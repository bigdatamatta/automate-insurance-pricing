<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>automate_insurance_pricing.risk_prediction.model_selection API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>automate_insurance_pricing.risk_prediction.model_selection</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import numpy as np

from sklearn.model_selection import cross_val_score, RandomizedSearchCV, KFold
from sklearn.feature_selection import RFECV, SelectFromModel

from hyperopt import fmin, hp, STATUS_OK, tpe, Trials
from hyperopt.pyll.stochastic import sample

import csv

from copy import deepcopy
import random

from timeit import default_timer as timer

from automate_insurance_pricing.risk_prediction.charts_functions import *


def hyperopt_obj_func(params, X_train=None, y_train=None, model=None, objective=None, scoring=None, file_path=None):
    &#34;&#34;&#34;
        Objective function for the bayesian hyperparameters search with hyperopt module   
        Arguments --&gt; the hyperparameters to try, the independent and dependent variables,   
            the algorithmn model to work on, its objective function and the scoring method to evaluate which params give the best model   
            If a scoring method is indicated it will overwrite the objective function   
            the file path where the results will be saved
        Returns --&gt; a dict storing the hyperopt trials (loss and status) and the params
    &#34;&#34;&#34;

    params[&#39;subsample&#39;] = params[&#39;boosting_type&#39;].get(&#39;subsample&#39;, 1.0)
    params[&#39;boosting_type&#39;] = params[&#39;boosting_type&#39;][&#39;boosting_type&#39;]

    selected_model = model(**params) if objective is None else model(**params, objective=objective)
    loss = cross_val_score(selected_model, X_train, y_train, scoring=scoring).mean()

    try:
        of_connection = open(file_path, &#39;a&#39;)
        writer = csv.writer(of_connection)
        writer.writerow([&#39;bayesian_optim&#39;, loss, params])
    except:
        print(&#39;The results could not be written in the csv. Check the file path.&#39;)
        pass

    return {&#39;loss&#39;: loss, &#39;params&#39;: params, &#39;status&#39;: STATUS_OK}


def run_random_search(X_train, y_train, model, param_grid, n_iter=10, cv=5, scoring=&#39;neg_root_mean_squared_error&#39;, random_state=42, file_path=None, **params):
    &#34;&#34;&#34;
        Runs a Randomized hyperparameters search thanks to scikit learn method   
        Arguments --&gt; the features/target variable,   
            the model and params to fine tune, the number of iterations and folds for cross validations   
            the scoring method to select the best params, the random state to reproduce the same results   
            the file path where the results will be saved   
            and the keyword arguments for the model itself   
        Returns --&gt; the final params results
    &#34;&#34;&#34;

    rscv = RandomizedSearchCV(model(**params), param_grid, n_iter=n_iter, cv=cv, scoring=scoring, refit=True, random_state=random_state)
    rscv.fit(X_train, y_train)
    
    try:
        of_connection = open(file_path, &#39;a&#39;)
        writer = csv.writer(of_connection)
        writer.writerow([&#39;random_search&#39;, rscv.best_score_, rscv.best_params_])
    except:
        print(&#39;The results could not be written in the csv. Check the file path.&#39;)
        pass

    return rscv


# def get_random_results(lgb_train_set, params, iteration, metrics=&#39;mape&#39;, stratified=False, random_state=42, **kwargs):
#     &#34;&#34;&#34;
#         Gets the model results using the hyperparameters and cross validation method
#         Arguments --&gt; the hyperparameters chosen, the iteration index and the number of folds used for cross validation
#     &#34;&#34;&#34;

#     start_time = timer()

#     cv_results = lgb.cv(params, lgb_train_set, metrics=metrics, stratified=stratified, seed=random_state, **kwargs)
#     end_time = timer()

#     loss = 1 - np.max(cv_results[metrics + &#39;-mean&#39;])
#     n_estimators = int(np.argmax(cv_results[metrics + &#39;-mean&#39;]) + 1)

#     return [loss, params, iteration, n_estimators, end_time - start_time]



# def run_random_search(df_random_results, lgb_train_set, param_grid, max_evals, subsample_dist=None, **kwargs):
#     &#34;&#34;&#34;
#         Perfoms a randomized search to find the best hyperparameter for a specified model

#     &#34;&#34;&#34;

#     for i in range(max_evals):
#         random_params = {key: random.sample(value, 1)[0] for key, value in param_grid.items()}
#         random_params[&#39;subsample&#39;] = random.sample(subsample_dist, 1)[0] if subsample_dist is not None and random_params[&#39;boosting_type&#39;] != &#39;goss&#39; else 1.0

#     random_results = get_random_results(lgb_train_set, random_params, i, **kwargs)

#     df_random_results.loc[i, :] = random_results



def run_multiple_models(X_train, y_train, models, y_transformer=None, scoring=&#39;accuracy&#39;, n_splits=5, plot_results=True, random_state=42):
    &#34;&#34;&#34;
        Runs multiple models to compare their results   
        Arguments --&gt; The features, the target variable, the models to run,   
            the transformer if the target variable has been transformed (e.g. a log transformation),   
            the scoring measure to use to compare the algorithms,   
            the number of folds for the cross validation,   
            a boolean indicating if a plot of the result will be displayed,   
            a random state to reproduce the same results   
        Returns --&gt; the results of the models
    &#34;&#34;&#34;
    results = []
    models_tried = [model[1] for model in models]
    names = [model[0] for model in models]

    y = deepcopy(y_train)

    if y_transformer is not None:
        y = y_transformer.transform(y)

    for name, model in models:
        kfold = KFold(n_splits=n_splits)
        cv_results = cross_val_score(model(), X_train, y, cv=kfold, scoring=scoring)
        results.append(cv_results)
        msg = &#34;%s: %f (%f)&#34; % (name, cv_results.mean(), cv_results.std())

    if plot_results == True:
        plot_models_results(results, names)

    return list(zip(names, models_tried, results))



def get_mape(y_true, y_pred):
    &#34;&#34;&#34; Derives the mean absolute percentage error between the actual and predicted value&#34;&#34;&#34;

    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.where(y_true != 0, np.abs((y_true - y_pred) / y_true) * 100, 0))



def mean_percentage_error(y_true, y_pred):
    &#34;&#34;&#34; Derives the mean percentage error between the actual and predicted value&#34;&#34;&#34;

    y_true, y_pred = np.array(y_true), np.array(y_pred)
    nil_y_error = ((y_true - y_pred) / y_true)[y_true!=0]

    return round(np.mean(nil_y_error) * 100, 2)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="automate_insurance_pricing.risk_prediction.model_selection.get_mape"><code class="name flex">
<span>def <span class="ident">get_mape</span></span>(<span>y_true, y_pred)</span>
</code></dt>
<dd>
<div class="desc"><p>Derives the mean absolute percentage error between the actual and predicted value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_mape(y_true, y_pred):
    &#34;&#34;&#34; Derives the mean absolute percentage error between the actual and predicted value&#34;&#34;&#34;

    y_true, y_pred = np.array(y_true), np.array(y_pred)
    return np.mean(np.where(y_true != 0, np.abs((y_true - y_pred) / y_true) * 100, 0))</code></pre>
</details>
</dd>
<dt id="automate_insurance_pricing.risk_prediction.model_selection.hyperopt_obj_func"><code class="name flex">
<span>def <span class="ident">hyperopt_obj_func</span></span>(<span>params, X_train=None, y_train=None, model=None, objective=None, scoring=None, file_path=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Objective function for the bayesian hyperparameters search with hyperopt module <br>
Arguments &ndash;&gt; the hyperparameters to try, the independent and dependent variables, <br>
the algorithmn model to work on, its objective function and the scoring method to evaluate which params give the best model <br>
If a scoring method is indicated it will overwrite the objective function <br>
the file path where the results will be saved
Returns &ndash;&gt; a dict storing the hyperopt trials (loss and status) and the params</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hyperopt_obj_func(params, X_train=None, y_train=None, model=None, objective=None, scoring=None, file_path=None):
    &#34;&#34;&#34;
        Objective function for the bayesian hyperparameters search with hyperopt module   
        Arguments --&gt; the hyperparameters to try, the independent and dependent variables,   
            the algorithmn model to work on, its objective function and the scoring method to evaluate which params give the best model   
            If a scoring method is indicated it will overwrite the objective function   
            the file path where the results will be saved
        Returns --&gt; a dict storing the hyperopt trials (loss and status) and the params
    &#34;&#34;&#34;

    params[&#39;subsample&#39;] = params[&#39;boosting_type&#39;].get(&#39;subsample&#39;, 1.0)
    params[&#39;boosting_type&#39;] = params[&#39;boosting_type&#39;][&#39;boosting_type&#39;]

    selected_model = model(**params) if objective is None else model(**params, objective=objective)
    loss = cross_val_score(selected_model, X_train, y_train, scoring=scoring).mean()

    try:
        of_connection = open(file_path, &#39;a&#39;)
        writer = csv.writer(of_connection)
        writer.writerow([&#39;bayesian_optim&#39;, loss, params])
    except:
        print(&#39;The results could not be written in the csv. Check the file path.&#39;)
        pass

    return {&#39;loss&#39;: loss, &#39;params&#39;: params, &#39;status&#39;: STATUS_OK}</code></pre>
</details>
</dd>
<dt id="automate_insurance_pricing.risk_prediction.model_selection.mean_percentage_error"><code class="name flex">
<span>def <span class="ident">mean_percentage_error</span></span>(<span>y_true, y_pred)</span>
</code></dt>
<dd>
<div class="desc"><p>Derives the mean percentage error between the actual and predicted value</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mean_percentage_error(y_true, y_pred):
    &#34;&#34;&#34; Derives the mean percentage error between the actual and predicted value&#34;&#34;&#34;

    y_true, y_pred = np.array(y_true), np.array(y_pred)
    nil_y_error = ((y_true - y_pred) / y_true)[y_true!=0]

    return round(np.mean(nil_y_error) * 100, 2)</code></pre>
</details>
</dd>
<dt id="automate_insurance_pricing.risk_prediction.model_selection.run_multiple_models"><code class="name flex">
<span>def <span class="ident">run_multiple_models</span></span>(<span>X_train, y_train, models, y_transformer=None, scoring='accuracy', n_splits=5, plot_results=True, random_state=42)</span>
</code></dt>
<dd>
<div class="desc"><p>Runs multiple models to compare their results <br>
Arguments &ndash;&gt; The features, the target variable, the models to run, <br>
the transformer if the target variable has been transformed (e.g. a log transformation), <br>
the scoring measure to use to compare the algorithms, <br>
the number of folds for the cross validation, <br>
a boolean indicating if a plot of the result will be displayed, <br>
a random state to reproduce the same results <br>
Returns &ndash;&gt; the results of the models</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_multiple_models(X_train, y_train, models, y_transformer=None, scoring=&#39;accuracy&#39;, n_splits=5, plot_results=True, random_state=42):
    &#34;&#34;&#34;
        Runs multiple models to compare their results   
        Arguments --&gt; The features, the target variable, the models to run,   
            the transformer if the target variable has been transformed (e.g. a log transformation),   
            the scoring measure to use to compare the algorithms,   
            the number of folds for the cross validation,   
            a boolean indicating if a plot of the result will be displayed,   
            a random state to reproduce the same results   
        Returns --&gt; the results of the models
    &#34;&#34;&#34;
    results = []
    models_tried = [model[1] for model in models]
    names = [model[0] for model in models]

    y = deepcopy(y_train)

    if y_transformer is not None:
        y = y_transformer.transform(y)

    for name, model in models:
        kfold = KFold(n_splits=n_splits)
        cv_results = cross_val_score(model(), X_train, y, cv=kfold, scoring=scoring)
        results.append(cv_results)
        msg = &#34;%s: %f (%f)&#34; % (name, cv_results.mean(), cv_results.std())

    if plot_results == True:
        plot_models_results(results, names)

    return list(zip(names, models_tried, results))</code></pre>
</details>
</dd>
<dt id="automate_insurance_pricing.risk_prediction.model_selection.run_random_search"><code class="name flex">
<span>def <span class="ident">run_random_search</span></span>(<span>X_train, y_train, model, param_grid, n_iter=10, cv=5, scoring='neg_root_mean_squared_error', random_state=42, file_path=None, **params)</span>
</code></dt>
<dd>
<div class="desc"><p>Runs a Randomized hyperparameters search thanks to scikit learn method <br>
Arguments &ndash;&gt; the features/target variable, <br>
the model and params to fine tune, the number of iterations and folds for cross validations <br>
the scoring method to select the best params, the random state to reproduce the same results <br>
the file path where the results will be saved <br>
and the keyword arguments for the model itself <br>
Returns &ndash;&gt; the final params results</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_random_search(X_train, y_train, model, param_grid, n_iter=10, cv=5, scoring=&#39;neg_root_mean_squared_error&#39;, random_state=42, file_path=None, **params):
    &#34;&#34;&#34;
        Runs a Randomized hyperparameters search thanks to scikit learn method   
        Arguments --&gt; the features/target variable,   
            the model and params to fine tune, the number of iterations and folds for cross validations   
            the scoring method to select the best params, the random state to reproduce the same results   
            the file path where the results will be saved   
            and the keyword arguments for the model itself   
        Returns --&gt; the final params results
    &#34;&#34;&#34;

    rscv = RandomizedSearchCV(model(**params), param_grid, n_iter=n_iter, cv=cv, scoring=scoring, refit=True, random_state=random_state)
    rscv.fit(X_train, y_train)
    
    try:
        of_connection = open(file_path, &#39;a&#39;)
        writer = csv.writer(of_connection)
        writer.writerow([&#39;random_search&#39;, rscv.best_score_, rscv.best_params_])
    except:
        print(&#39;The results could not be written in the csv. Check the file path.&#39;)
        pass

    return rscv</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="automate_insurance_pricing.risk_prediction" href="index.html">automate_insurance_pricing.risk_prediction</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="automate_insurance_pricing.risk_prediction.model_selection.get_mape" href="#automate_insurance_pricing.risk_prediction.model_selection.get_mape">get_mape</a></code></li>
<li><code><a title="automate_insurance_pricing.risk_prediction.model_selection.hyperopt_obj_func" href="#automate_insurance_pricing.risk_prediction.model_selection.hyperopt_obj_func">hyperopt_obj_func</a></code></li>
<li><code><a title="automate_insurance_pricing.risk_prediction.model_selection.mean_percentage_error" href="#automate_insurance_pricing.risk_prediction.model_selection.mean_percentage_error">mean_percentage_error</a></code></li>
<li><code><a title="automate_insurance_pricing.risk_prediction.model_selection.run_multiple_models" href="#automate_insurance_pricing.risk_prediction.model_selection.run_multiple_models">run_multiple_models</a></code></li>
<li><code><a title="automate_insurance_pricing.risk_prediction.model_selection.run_random_search" href="#automate_insurance_pricing.risk_prediction.model_selection.run_random_search">run_random_search</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>